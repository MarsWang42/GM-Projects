{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project we are trying to build a SSVM model for part-of- speech tagging in text data.\n",
    "\n",
    "To begin with, let's import only one sentence file and play with it. Our first goal is trying to calculate its loss and gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "datapath = 'data/';\n",
    "files = os.listdir(datapath)\n",
    "s = 50;   # to load file number 50:\n",
    "fh = open(datapath+files[s],'r');\n",
    "rawlines = fh.readlines();\n",
    "lines = [line.strip('\\n').split(',') for line in rawlines];\n",
    "fh.close();\n",
    "ys = [int(l[1])-1 for l in lines];\n",
    "xs = [[int(l[2])-1,int(l[3]),int(l[4]),int(l[5])-1,int(l[6])-1] for l in lines];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initialize necessary variables\n",
    "import numpy as np\n",
    "import pyGM as gm\n",
    "N = 0\n",
    "nt = 0.01\n",
    "Hamming_Loss = 0\n",
    "Hinge_Loss = 0\n",
    "feature_sizes = [1,2,2,201,201]\n",
    "ThetaF = [.001*np.random.rand(10,feature_sizes[f]) for f in range(len(feature_sizes))]; \n",
    "ThetaP = .001*np.random.rand(10,10);\n",
    "Loss = 1.0 - np.eye(10);  # hamming loss\n",
    "ns = len(ys)\n",
    "# Define random variables for the inference process:\n",
    "Y = [gm.Var(i,10) for i in range(ns)];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we need to build a pridction model. All parameters have been initiated in the matrixes ThetaF and ThetaP. Also yhat and ypred can be easily found by junctiontree function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Build \"prediction model\" using your parameters\n",
    "factors=[gm.Factor([Y[i],Y[i+1]],1.0) for i in range(ns-1)]\n",
    "for f in factors:\n",
    "    f.table = np.exp(ThetaP)\n",
    "for i in range(ns):\n",
    "    for j in range(len(feature_sizes)):\n",
    "        factor = gm.Factor(Y[i], 1)\n",
    "        factor.table = np.exp(ThetaF[j][:,xs[i][j]])\n",
    "        factors.append(factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pyGM.wmb\n",
    "# so exponentiate the factors before making a model... \n",
    "model_pred = gm.GraphModel(factors);\n",
    "# Copy factors and add extra Hamming factors for loss-augmented model\n",
    "factors_aug = [ f for f in factors ]\n",
    "factors_aug.extend( [gm.Factor([Y[i]], Loss[:,ys[i]]).exp() for i in range(ns)] ); \n",
    "model_aug = gm.GraphModel(factors_aug);\n",
    "\n",
    "order = range(ns);  # eliminate in sequence (Markov chain on y)\n",
    "wt = 1e-4;         # for max elimination in JTree implementation\n",
    "\n",
    "# Now, the most likely configuration of the prediction model (for prediction) is:\n",
    "yhat_pred = gm.wmb.JTree(model_pred,order,wt).argmax();\n",
    "# and the maximizing argument of the loss (for computing the gradient) is\n",
    "yhat_aug = gm.wmb.JTree(model_aug,order,wt).argmax();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is calculate the feature function u, which is the score of the permutation of y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ypred = list(yhat_pred.values())\n",
    "yhat = list(yhat_aug.values())\n",
    "h_loss = Loss[ypred, ys].sum()\n",
    "Hamming_Loss += h_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "u_ypred = 0\n",
    "for i in range(ns):\n",
    "    if i != ns-1:\n",
    "        u_ypred += ThetaP[ypred[i]][ypred[i+1]]\n",
    "    for j in range(len(feature_sizes)):\n",
    "        u_ypred += ThetaF[j][ypred[i]][xs[i][j]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "u_ys = 0\n",
    "for i in range(ns):\n",
    "    if i != ns-1:\n",
    "        u_ys += ThetaP[ys[i]][ys[i+1]]\n",
    "    for j in range(len(feature_sizes)):\n",
    "        u_ys += ThetaF[j][ys[i]][xs[i][j]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value of feature function is quite reasonable. The value of ys' feature function should be no greater than that of ypred's feature function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.065173994864121218"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u_ypred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.050951620491576248"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u_ys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last but not least, we can work on the calculation of gradient and update all of the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from numpy import linalg\n",
    "norms = [linalg.norm(ThetaF[i]) for i in range(len(feature_sizes))]\n",
    "norms.append(linalg.norm(ThetaP))\n",
    "norms = np.asarray(norms)\n",
    "Hinge_Loss += h_loss + u_ypred - u_ys + nt*np.sum(norms*norms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.014236576843304"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Hinge_Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N += ns\n",
    "GradP = np.zeros((10, 10))\n",
    "GradF = [np.zeros((10,feature_sizes[f])) for f in range(len(feature_sizes))];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(ns):\n",
    "    for j in range(len(feature_sizes)):\n",
    "        GradF[j][yhat[i],xs[i][j]] += 1\n",
    "        GradF[j][ys[i],xs[i][j]] -= 1\n",
    "    if i<ns-1:\n",
    "        GradP[yhat[i],yhat[i+1]] += 1\n",
    "        GradP[ys[i],ys[i+1]] -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "GradF = [nt*ThetaF[i] + GradF[i] for i in range(len(ThetaF))]\n",
    "GradP = nt*ThetaP + GradP\n",
    "\n",
    "ThetaF = [ThetaF[i] - GradF[i]/ns for i in range(len(ThetaF))]\n",
    "ThetaP = ThetaP - (1/ns)*GradP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done! We succesfully handled one of the sentence files. Now we can make an iteration on all of the sentence files and see how our algorithm performs!\n",
    "\n",
    "Due to the huge number of files, we only go through one iteration just to check the performance of our algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 1000th sentence.\n",
      "Iter: 1 Accuracy: 0.7588936244237627\n",
      "Average Hinge Loss: 7.197256453910526 Average Hamming Loss: 5.544\n",
      "Reading 2000th sentence.\n",
      "Iter: 1 Accuracy: 0.7926091586794463\n",
      "Average Hinge Loss: 6.830015123644105 Average Hamming Loss: 4.8685\n",
      "Reading 3000th sentence.\n",
      "Iter: 1 Accuracy: 0.8080422950447896\n",
      "Average Hinge Loss: 6.675682171007223 Average Hamming Loss: 4.514333333333333\n",
      "Reading 4000th sentence.\n",
      "Iter: 1 Accuracy: 0.8176956217237208\n",
      "Average Hinge Loss: 6.604635788033613 Average Hamming Loss: 4.295\n",
      "Reading 5000th sentence.\n",
      "Iter: 1 Accuracy: 0.8246810102263744\n",
      "Average Hinge Loss: 6.542542413335591 Average Hamming Loss: 4.1248\n",
      "Reading 6000th sentence.\n",
      "Iter: 1 Accuracy: 0.8296297874455831\n",
      "Average Hinge Loss: 6.510535461860795 Average Hamming Loss: 3.9983333333333335\n"
     ]
    }
   ],
   "source": [
    "import pyGM as gm\n",
    "import pyGM.wmb\n",
    "import numpy as np\n",
    "import os\n",
    "from numpy import linalg\n",
    "\n",
    "datapath = 'data/';\n",
    "files = os.listdir(datapath)\n",
    "feature_sizes = [1,2,2,201,201]\n",
    "ThetaF = [.001*np.random.rand(10,feature_sizes[f]) for f in range(len(feature_sizes))]; \n",
    "ThetaP = .001*np.random.rand(10,10);\n",
    "Loss = 1.0 - np.eye(10);  # hamming loss\n",
    "step = 0.1\n",
    "reg = 0.01\n",
    "num_iter = 1\n",
    "HinLs = []\n",
    "HamLs = []\n",
    "# step size, etc.\n",
    "for iter in range(num_iter):\n",
    "    num = 0\n",
    "    correct = 0\n",
    "    N = 0\n",
    "    Hamming_Loss = 0\n",
    "    Hinge_Loss = 0\n",
    "    for s in np.random.permutation(len(files)-1):\n",
    "        num+=1\n",
    "        if(num%100 == 0):\n",
    "            accuracy = 1-Hamming_Loss/N\n",
    "            avg_HinL = Hinge_Loss/num\n",
    "            avg_HamL = Hamming_Loss/num\n",
    "            HinLs.append(avg_HinL)\n",
    "            HamLs.append(avg_HamL)\n",
    "        if(num%1000 == 0):\n",
    "            print('Reading '+str(num)+'th sentence.')\n",
    "            print(\"Iter: {} Accuracy: {}\".format(iter+1, accuracy))\n",
    "            print(\"Average Hinge Loss: {} Average Hamming Loss: {}\".format(avg_HinL, avg_HamL))\n",
    "        # Load data ys,xs\n",
    "        fh = open(datapath+files[s+1],'r');\n",
    "        rawlines = fh.readlines();\n",
    "        lines = [line.strip('\\n').split(',') for line in rawlines];\n",
    "        fh.close();\n",
    "        ys = [int(l[1])-1 for l in lines];\n",
    "        xs = [[int(l[2])-1,int(l[3]),int(l[4]),int(l[5])-1,int(l[6])-1] for l in lines];\n",
    "        ns = len(ys)\n",
    "        N += ns\n",
    "        # Define random variables for the inference process:\n",
    "        Y = [gm.Var(i,10) for i in range(ns)];\n",
    "\n",
    "        # Build \"prediction model\" using your parameters\n",
    "        factors=[gm.Factor([Y[i],Y[i+1]],1.0) for i in range(ns-1)]\n",
    "        for f in factors:\n",
    "            f.table = np.exp(ThetaP)\n",
    "        for i in range(ns):\n",
    "            for j in range(len(feature_sizes)):\n",
    "                factor = gm.Factor(Y[i], 1)\n",
    "                factor.table = np.exp(ThetaF[j][:,xs[i][j]])\n",
    "                factors.append(factor)\n",
    "        \n",
    "        # don't forget pyGM expects models to be products of factors, \n",
    "        # so exponentiate the factors before making a model... \n",
    "        model_pred = gm.GraphModel(factors);\n",
    "        # Copy factors and add extra Hamming factors for loss-augmented model\n",
    "        factors_aug = [ f for f in factors ]\n",
    "        factors_aug.extend( [gm.Factor([Y[i]], Loss[:,ys[i]]).exp() for i in range(ns)] ); \n",
    "        model_aug = gm.GraphModel(factors_aug);\n",
    "\n",
    "        order = range(ns);  # eliminate in sequence (Markov chain on y)\n",
    "        wt = 1e-4;         # for max elimination in JTree implementation\n",
    "\n",
    "        # Now, the most likely configuration of the prediction model (for prediction) is:\n",
    "        yhat_pred = gm.wmb.JTree(model_pred,order,wt).argmax();\n",
    "        # and the maximizing argument of the loss (for computing the gradient) is\n",
    "        yhat_aug = gm.wmb.JTree(model_aug,order,wt).argmax();\n",
    "\n",
    "        # use yhat_pred & ys to keep a running estimate of your prediction accuracy & printit\n",
    "        ypred = list(yhat_pred.values())\n",
    "        yhat = list(yhat_aug.values())\n",
    "        h_loss = Loss[ypred, ys].sum()\n",
    "        Hamming_Loss += h_loss\n",
    "        # calculating feature function\n",
    "        u_ypred = 0\n",
    "        for i in range(ns):\n",
    "            if i != ns-1:\n",
    "                u_ypred += ThetaP[ypred[i]][ypred[i+1]]\n",
    "            for j in range(len(feature_sizes)):\n",
    "                u_ypred += ThetaF[j][ypred[i]][xs[i][j]]\n",
    "        u_ys = 0\n",
    "        for i in range(ns):\n",
    "            if i != ns-1:\n",
    "                u_ys += ThetaP[ys[i]][ys[i+1]]\n",
    "            for j in range(len(feature_sizes)):\n",
    "                u_ys += ThetaF[j][ys[i]][xs[i][j]]\n",
    "                \n",
    "        # calculating Hinge Loss\n",
    "        norms = [linalg.norm(ThetaF[i]) for i in range(len(feature_sizes))]\n",
    "        norms.append(linalg.norm(ThetaP))\n",
    "        norms = np.asarray(norms)\n",
    "        Hinge_Loss += Loss[ypred, ys].sum() + u_ypred - u_ys + reg*np.dot(norms, norms)\n",
    "        # how often etc is up to you\n",
    "        # use yhat_aug & ys to update your parameters theta in the negative gradient direction\n",
    "        GradP = np.zeros((10, 10))\n",
    "        GradF = [np.zeros((10,feature_sizes[f])) for f in range(len(feature_sizes))];\n",
    "        for i in range(ns):\n",
    "            for j in range(len(feature_sizes)):\n",
    "                GradF[j][yhat[i],xs[i][j]] += 1\n",
    "                GradF[j][ys[i],xs[i][j]] -= 1\n",
    "            if i<ns-1:\n",
    "                GradP[yhat[i],yhat[i+1]] += 1\n",
    "                GradP[ys[i],ys[i+1]] -= 1\n",
    "        GradF = [reg*ThetaF[i] + GradF[i] for i in range(len(ThetaF))]\n",
    "        GradP = reg*ThetaP + GradP\n",
    "        ThetaF = [ThetaF[i] - step*GradF[i]/ns for i in range(len(ThetaF))]\n",
    "        ThetaP = ThetaP - step*(1/ns)*GradP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last but not least, show the beautiful graph of Hinge loss and Hamming loss we've got during the SGD! We can see it is monotonically decreasing :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlMAAAHpCAYAAACm+LlmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10becc438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# a magic to inline matplot\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "plt.plot([i for i in range(len(HinLs))], HinLs)\n",
    "plt.plot([i for i in range(len(HamLs))], HamLs)\n",
    "plt.title('Hinge Loss & Hamming Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "A = np.array([[1/3, 2/3,0,0,0,0,0], [1/4, 3/4,0,0,0,0,0], [0,0,0,2/3,1/3,0,0], [0,0,1,0,0,0,0], [0,0,1,0,0,0,0],\n",
    "             [1/6,0,1/6,1/6,0,1/4,1/4], [0,0,0,0,0,0,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  2.72727273e-001   7.27272727e-001   0.00000000e+000   0.00000000e+000\n",
      "    0.00000000e+000   0.00000000e+000   0.00000000e+000]\n",
      " [  2.72727273e-001   7.27272727e-001   0.00000000e+000   0.00000000e+000\n",
      "    0.00000000e+000   0.00000000e+000   0.00000000e+000]\n",
      " [  0.00000000e+000   0.00000000e+000   1.00000000e+000   0.00000000e+000\n",
      "    0.00000000e+000   0.00000000e+000   0.00000000e+000]\n",
      " [  0.00000000e+000   0.00000000e+000   0.00000000e+000   6.66666667e-001\n",
      "    3.33333333e-001   0.00000000e+000   0.00000000e+000]\n",
      " [  0.00000000e+000   0.00000000e+000   0.00000000e+000   6.66666667e-001\n",
      "    3.33333333e-001   0.00000000e+000   0.00000000e+000]\n",
      " [  6.06060606e-002   1.61616162e-001   2.22222222e-001   1.48148148e-001\n",
      "    7.40740741e-002   7.45834073e-155   3.33333333e-001]\n",
      " [  0.00000000e+000   0.00000000e+000   0.00000000e+000   0.00000000e+000\n",
      "    0.00000000e+000   0.00000000e+000   1.00000000e+000]]\n"
     ]
    }
   ],
   "source": [
    "A = A.dot(A)\n",
    "print (A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  2.72727273e-001   7.27272727e-001   0.00000000e+000   0.00000000e+000\n",
      "    0.00000000e+000   0.00000000e+000   0.00000000e+000]\n",
      " [  2.72727273e-001   7.27272727e-001   0.00000000e+000   0.00000000e+000\n",
      "    0.00000000e+000   0.00000000e+000   0.00000000e+000]\n",
      " [  0.00000000e+000   0.00000000e+000   0.00000000e+000   6.66666667e-001\n",
      "    3.33333333e-001   0.00000000e+000   0.00000000e+000]\n",
      " [  0.00000000e+000   0.00000000e+000   1.00000000e+000   0.00000000e+000\n",
      "    0.00000000e+000   0.00000000e+000   0.00000000e+000]\n",
      " [  0.00000000e+000   0.00000000e+000   1.00000000e+000   0.00000000e+000\n",
      "    0.00000000e+000   0.00000000e+000   0.00000000e+000]\n",
      " [  6.06060606e-002   1.61616162e-001   2.22222222e-001   1.48148148e-001\n",
      "    7.40740741e-002   1.16536574e-156   3.33333333e-001]\n",
      " [  0.00000000e+000   0.00000000e+000   0.00000000e+000   0.00000000e+000\n",
      "    0.00000000e+000   0.00000000e+000   1.00000000e+000]]\n"
     ]
    }
   ],
   "source": [
    "A = A.dot(np.array([[1/3, 2/3,0,0,0,0,0], [1/4, 3/4,0,0,0,0,0], [0,0,0,2/3,1/3,0,0], [0,0,1,0,0,0,0], [0,0,1,0,0,0,0],\n",
    "             [1/6,0,1/6,1/6,0,1/4,1/4], [0,0,0,0,0,0,1]]))\n",
    "print (A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
